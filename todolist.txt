===============================================================================
Write-up requirements
===============================================================================
(a) intro describing at a high level some approaches that you considered, and why

(b) description of your submitted solution, including any data processing, algo-
rithms used, etc.

(c) section describing some empirical results from your solution
    - i.e., experiments that demonstrate that your solution is sensible
    - include a comparison of your approach against at LEAST TWO other approaches
      that you have tried, with one of them being the kNN baseline
    - may also include:
        - an evaluation of a regularization technique to prevent
          overfitting
        - a comparison of different model parameters (such as tanh vs. sig-
          moid neurons/the number of neurons in a neural network, or different ker-
          nels/kernel parameter settings in an SVM)
        - the effects of using increasing amounts of unlabeled data in a 
          semi-supervised learning algorithm
        - report of using cross validation
        - report of statistical tests to prove the significance of your results
        - argue that your way is the logical way to find and select the best hyperparameters
          
    JUSTIFY YOUR APPROACH; especially justify with respect to this specific dataset.
        You should explain for which cases your method worked well and for which cases it failed and why.
        You should comment on your overall performance and explain why is it high/low.
        You should also explain how you handled unlabeled data (justify why use or not use).
        You should also try different variations of your method (regularization, 
        different kernels/neuron types/distance metrics etc.) and comment on what changes.
    
    Use lots of plots and tables.

(d) conclusion summarizing your work and findings
    - if your method performed poorly on the public or hidden test data, explain
      and suggest improvements

(e) references to any code, methods, or ideas that you used that are not your own

(f) any special instructions that are required to run your code
===============================================================================
Kaggle file submission requirements
===============================================================================
- make sure to add a header!

===============================================================================
Data
===============================================================================
- 2925 labeled images, 98 058 unlabeled images:
    - 32 × 32 grayscale
    - labels: 1-Anger, 2-Disgust, 3-Fear, 4-Happy, 5-Sad, 6-Surprise, 7-Neutral

===============================================================================
Pre-processing
===============================================================================
- subtract the mean RGB value, computed on the training set, from each pixel \cite{SimonyanZ14a}
- whitening the data?


===============================================================================
Validation
===============================================================================
- note that each labeled face image has an identity, and there are different expres-
  sions associated with each person in the labeled set. When you split the labeled set to
  perform cross-validation on hyper parameter etc., you should make sure that people with
  the same identity are not put into both training and validation set. Having one person’s
  face in both training and validation introduces a large noise.

- if dataset is unbalanced, and has a large majority
  of examples in one class, the classifier may learn
  to always predict the most frequent class, which still gives low
  classification error.
  For instance, if 96% of the examples are of class 1, then always
  predicting 1 will give 4% error.
  
  To check if it is the case, you can compile a function that only
  computes the classification error given the prediction (not the input!)
  and the actual target. Then, call that function on your "result" array,
  and check if you have the same error rate. 


===============================================================================
Architecture
===============================================================================
- decay the learning weight in each epoch






===============================================================================
References
===============================================================================
@article{DBLP:journals/corr/SimonyanZ14a,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal   = {CoRR},
  year      = {2014},
  volume    = {abs/1409.1556},
  url       = {http://arxiv.org/abs/1409.1556},
  timestamp = {Thu, 13 Nov 2014 01:15:09 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
